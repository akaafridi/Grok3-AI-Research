{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9xzXMqjQIMK",
        "outputId": "46f9c04b-049a-423c-dd8c-79abee7dd039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== QLoRA vs Full Fine-Tuning Simulation Results ===\n",
            "Full Fine-Tuning Step Time: 6.413243 sec\n",
            "QLoRA Fine-Tuning Step Time: 0.660742 sec\n",
            "Trainable Parameters (Full Fine-Tune): 10492928\n",
            "Trainable Parameters (QLoRA): 8426496\n",
            "Simulated VRAM Reduction: 19.69%\n",
            "\n",
            "Results saved to: qlora_experiments/qlora_vs_fullfinetune_results.txt\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Create folder if not exists\n",
        "os.makedirs(\"qlora_experiments\", exist_ok=True)\n",
        "\n",
        "# Simulate a small Transformer Block\n",
        "class TinyTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_dim * 4, embed_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = self.q_proj(x)\n",
        "        v = self.v_proj(x)\n",
        "        ffn_out = self.ffn(x)\n",
        "        return q + v + ffn_out\n",
        "\n",
        "# LoRA Adapter applied to Linear layer\n",
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, original_layer, rank=8):\n",
        "        super().__init__()\n",
        "        self.original_layer = original_layer\n",
        "        self.lora_A = nn.Linear(original_layer.in_features, rank, bias=False)\n",
        "        self.lora_B = nn.Linear(rank, original_layer.out_features, bias=False)\n",
        "        # Freeze original layer\n",
        "        for param in self.original_layer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.original_layer(x) + self.lora_B(self.lora_A(x))\n",
        "\n",
        "# Apply LoRA Adapters\n",
        "def apply_lora(model, target_modules=[\"q_proj\", \"v_proj\"], rank=8):\n",
        "    for name, module in model.named_children():\n",
        "        if name in target_modules:\n",
        "            lora_module = LoRAAdapter(module, rank=rank)\n",
        "            setattr(model, name, lora_module)\n",
        "        else:\n",
        "            apply_lora(module, target_modules, rank)\n",
        "\n",
        "# Create dummy input\n",
        "batch_size = 8\n",
        "seq_len = 128\n",
        "embed_dim = 1024\n",
        "\n",
        "input_tensor = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Instantiate models\n",
        "full_model = TinyTransformer(embed_dim)\n",
        "qlora_model = TinyTransformer(embed_dim)\n",
        "apply_lora(qlora_model)\n",
        "\n",
        "# Training Step simulation (single forward+backward)\n",
        "def training_step(model, input_tensor):\n",
        "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
        "    model.train()\n",
        "    output = model(input_tensor)\n",
        "    loss = output.mean()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Benchmark Full Fine-Tuning\n",
        "start = time.time()\n",
        "training_step(full_model, input_tensor)\n",
        "full_finetune_time = time.time() - start\n",
        "\n",
        "# Benchmark QLoRA Fine-Tuning\n",
        "start = time.time()\n",
        "training_step(qlora_model, input_tensor)\n",
        "qlora_finetune_time = time.time() - start\n",
        "\n",
        "# Simulate VRAM usage (number of trainable params)\n",
        "def count_trainable_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "full_params = count_trainable_params(full_model)\n",
        "qlora_params = count_trainable_params(qlora_model)\n",
        "\n",
        "# Simulated Memory Savings\n",
        "vram_reduction = 100 * (1 - qlora_params / full_params)\n",
        "\n",
        "# Print results\n",
        "print(\"=== QLoRA vs Full Fine-Tuning Simulation Results ===\")\n",
        "print(f\"Full Fine-Tuning Step Time: {full_finetune_time:.6f} sec\")\n",
        "print(f\"QLoRA Fine-Tuning Step Time: {qlora_finetune_time:.6f} sec\")\n",
        "print(f\"Trainable Parameters (Full Fine-Tune): {full_params}\")\n",
        "print(f\"Trainable Parameters (QLoRA): {qlora_params}\")\n",
        "print(f\"Simulated VRAM Reduction: {vram_reduction:.2f}%\")\n",
        "\n",
        "# Save results to file\n",
        "result_text = f\"\"\"\n",
        "=== QLoRA vs Full Fine-Tuning Simulation Results ===\n",
        "\n",
        "Batch Size: {batch_size}\n",
        "Sequence Length: {seq_len}\n",
        "Embedding Dimension: {embed_dim}\n",
        "\n",
        "Full Fine-Tuning Step Time: {full_finetune_time:.6f} sec\n",
        "QLoRA Fine-Tuning Step Time: {qlora_finetune_time:.6f} sec\n",
        "\n",
        "Trainable Parameters (Full Fine-Tune): {full_params}\n",
        "Trainable Parameters (QLoRA Fine-Tune): {qlora_params}\n",
        "\n",
        "Simulated VRAM Reduction: {vram_reduction:.2f}%\n",
        "\"\"\"\n",
        "\n",
        "with open(\"qlora_experiments/qlora_vs_fullfinetune_results.txt\", \"w\") as f:\n",
        "    f.write(result_text)\n",
        "\n",
        "print(\"\\nResults saved to: qlora_experiments/qlora_vs_fullfinetune_results.txt\")\n"
      ]
    }
  ]
}